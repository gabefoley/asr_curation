Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job stats:
job                               count    min threads    max threads
------------------------------  -------  -------------  -------------
add_annotations_from_alignment        3              1              1
add_custom_annotations                1              1              1
align_seqs                            3              1              1
all                                   1              1              1
clean_subset_summary                  3              1              1
clean_summary_document                2              1              1
compile_summary_document              3              1              1
create_annotation_file                3              1              1
create_annotations                    1              1              1
create_dataset_summary                2              1              1
create_subset_document                3              1              1
create_subset_summary                 3              1              1
create_subsets                        1              1              1
get_brenda_annotations                1              1              1
get_uniprot_annotations               1              1              1
total                                31              1              1

Select jobs to execute...

[Mon Jun  6 09:23:48 2022]
rule create_dataset_summary:
    input: workflows/example_workflow/datasets/uniprot_ec_1_1_1_86_filtered_reviewed_yes/csv/custom/uniprot_ec_1_1_1_86_filtered_reviewed_yes_annotated.csv
    output: workflows/example_workflow/datasets/uniprot_ec_1_1_1_86_filtered_reviewed_yes/dataset_summary/temp/uniprot_ec_1_1_1_86_filtered_reviewed_yes_summary.ipynb
    log: workflows/example_workflow/datasets/uniprot_ec_1_1_1_86_filtered_reviewed_yes/dataset_summary/temp/uniprot_ec_1_1_1_86_filtered_reviewed_yes_summary.ipynb
    jobid: 24
    reason: Missing output files: workflows/example_workflow/datasets/uniprot_ec_1_1_1_86_filtered_reviewed_yes/dataset_summary/temp/uniprot_ec_1_1_1_86_filtered_reviewed_yes_summary.ipynb
    wildcards: dataset=uniprot_ec_1_1_1_86_filtered_reviewed_yes
    resources: tmpdir=/var/folders/xs/24s9hwqd191f2x7rhdy6_ryc0000gr/T

[Mon Jun  6 09:23:53 2022]
Error in rule create_dataset_summary:
    jobid: 24
    output: workflows/example_workflow/datasets/uniprot_ec_1_1_1_86_filtered_reviewed_yes/dataset_summary/temp/uniprot_ec_1_1_1_86_filtered_reviewed_yes_summary.ipynb
    log: workflows/example_workflow/datasets/uniprot_ec_1_1_1_86_filtered_reviewed_yes/dataset_summary/temp/uniprot_ec_1_1_1_86_filtered_reviewed_yes_summary.ipynb (check log file(s) for error message)

RuleException:
CalledProcessError in line 285 of /Users/uqgfoley/Documents/asr_curation_moved/asr_curation/Snakefile:
Command 'set -euo pipefail;  jupyter-nbconvert --log-level ERROR --execute --output /Users/uqgfoley/Documents/asr_curation_moved/asr_curation/workflows/example_workflow/datasets/uniprot_ec_1_1_1_86_filtered_reviewed_yes/dataset_summary/temp/uniprot_ec_1_1_1_86_filtered_reviewed_yes_summary.ipynb --to notebook --ExecutePreprocessor.timeout=-1 /Users/uqgfoley/Documents/asr_curation_moved/asr_curation/.snakemake/scripts/tmpdua_fw3q.create_summary_document.py.ipynb' returned non-zero exit status 1.
  File "/Users/uqgfoley/Documents/asr_curation_moved/asr_curation/Snakefile", line 285, in __rule_create_dataset_summary
  File "/Users/uqgfoley/opt/miniconda3/envs/cleanrepo/lib/python3.10/concurrent/futures/thread.py", line 58, in run
Removing output files of failed job create_dataset_summary since they might be corrupted:
workflows/example_workflow/datasets/uniprot_ec_1_1_1_86_filtered_reviewed_yes/dataset_summary/temp/uniprot_ec_1_1_1_86_filtered_reviewed_yes_summary.ipynb
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: .snakemake/log/2022-06-06T092348.197223.snakemake.log
