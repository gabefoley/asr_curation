{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ASR curation","text":""},{"location":"#asr-curation-overview","title":"ASR curation overview","text":"<p>ASR curation is a snakemake pipeline for annotating sequences to be used in ancestral sequence reconstruction.</p> <p>It implements a re-runnable workflow of all the basic steps of a reconstruction -</p> <ul> <li>sequence alignment </li> <li>tree inference</li> <li>ancestral prediction</li> </ul> <p>The focus is on building up a series of annotation files for all of the sequences in your dataset.</p> <p>The columns in these alignment files are used to subset the data by defining a series of rules including or excluding sequences.</p> <p>The annotations can be further used to visualise your data on phylogenetic trees and within sequence alignments.</p> <p> </p> Figure 1: Overview of ASR curation."},{"location":"#subsets","title":"Subsets","text":"<p>A single dataset can have many subsets. </p> <p>From Figure 1, there is one defined dataset (step 1) and so a master annotation file is built once (step 2). There are three defined subsets (step 3) and so alignment, tree inference, and ancestor prediction occurs three times - once per subset (step 4) resulting in three trees (step 5). This Figure defines three subsets (step 3) and so three trees are produced (step 5).</p> <p>Annotations can also be added after the alignment and ancestor stage, but currently these are just used for visualisation purposes, not for curation.</p> <p>The focus on subset rule files means that you can -</p> <ul> <li>create many different subsets at once and </li> <li>have a list of understandable and reproducible rules for curating your data </li> </ul>"},{"location":"#documentation-overview","title":"Documentation overview","text":"<ul> <li>Installation and quickstart <ul> <li>Installation - Shows how to install <code>asr_curation</code> locally and what files and commands run the pipeline. </li> <li>Running example data - Explains how to run the example data to verify the pipeline is working correctly </li> <li>Explanation of output - Shows what each of the output folders contain after running the example data</li> </ul> </li> <li>Detailed explanation <ul> <li>Basic snakemake - A quick introduction to key snakemake concepts and how to read the steps defined in the <code>snakefile</code> </li> <li>Explanation of steps - A detailed rundown of what each step within the <code>asr_curation</code> pipeline is doing </li> </ul> </li> <li>Customisation <ul> <li>Defining files - How to define the <code>config</code> and <code>subset</code> files so that you can run the <code>asr_curation</code> pipeline on your data </li> <li>Customising workflows - How to customise the workflow by adding custom annotations to be added to each dataset</li> </ul> </li> </ul>"},{"location":"#cool-features","title":"Cool features","text":"<p>The focus on annotations allows us to directly visualise these annotations -</p> <p> </p> Figure 2: Domains automatically retrieved from UniProt and overlaid on an alignment. <p> </p> Figure 3: All annotations can be added to custom annotation files viewable in FigTree <p> </p> Figure 4: Patterns of specific annotations can be viewed across subsets."},{"location":"basic_snakemake/","title":"Basic snakemake","text":""},{"location":"basic_snakemake/#snakefile","title":"Snakefile","text":"<p>The main workflow is contained within the <code>snakefile</code> file in the top folder of the <code>asr_curation</code> repository.</p> <p>A <code>snakefile</code> contains a series of rules that define -</p> <ul> <li>an input</li> <li>an output</li> <li>how to get to the output from the input, typically either a Python script or a shell command</li> </ul>"},{"location":"basic_snakemake/#rules-in-snakefiles","title":"Rules in snakefiles","text":"<p>For example, this rule defines -  </p> <ul> <li>the input as an alignment, </li> <li>the output as a tree file (in Newick format) </li> <li>a shell command to run in order to generate this - in this case calling <code>FastTree</code></li> </ul> <pre><code>rule infer_tree:\n    input:\n        WORKDIR + \"/{dataset}/subsets/{subset}/{dataset}_{subset}.aln\"\n\n    output:\n        WORKDIR + \"/{dataset}/subsets/{subset}/{dataset}_{subset}.nwk\"\n\n    shell:\n        \"FastTree {input} &gt; {output}\"\n</code></pre> <p>Anything defined within curly brackets <code>{}</code> is a wildcard. </p> <p>The wildcards in the shell command let us use the whatever we define as <code>input</code> and <code>output</code> within the shell command.</p> <p>The wildcards in the input and output rules let as define different <code>dataset</code> and <code>subset</code> wildcards which makes it possible to run the entire snakemake pipeline on different sets of data.</p>"},{"location":"basic_snakemake/#triggering-a-snakemake-pipeline","title":"Triggering a snakemake pipeline","text":"<p>Once you trigger a snakemake pipeline, it will look for the first command defined in the <code>snakefile</code> file and try to create the output.</p> <p>By convention, this is a rule called <code>all</code> which we place at the top of the <code>snakefile</code></p> <p>In <code>asr_curation</code> the following rule is defined as the first rule -</p> <pre><code>rule all:\n        input:\n            annotations = [f'{WORKDIR}/{dataset}/subsets/{subset}/csv/{dataset}_{subset}_annotations.txt' for dataset in DATASETS for subset in subsets[dataset]],\n            ancestors = [f'{WORKDIR}/{dataset}/subsets/{subset}/csv/{dataset}_{subset}_ancestors.csv' for dataset in DATASETS for subset in subsets[dataset]]\n</code></pre> <p>Snakemake will look to see if these <code>input</code> files already exists for each <code>dataset</code> and <code>subset</code> in terms of both the <code>annotations.txt</code> and the <code>ancestors.csv</code></p> <p>If these files do not exist, snakemake finds a rule defined in the <code>snakefile</code> that it would need to run in order to generate these files</p> <p>Ignoring the other detail, focus here on the output line - this rule will generate the output of <code>ancestors.csv</code> which is one of the files we needed as input to the <code>all</code> rule.</p> <p>If this file didn't already exist, then we know we must run this rule in order to generate it!</p> <pre><code>rule add_annotations_from_ancestors:\n    input:\n        csv = WORKDIR + \"/{dataset}/subsets/{subset}/csv/{dataset}_{subset}_alignment.csv\",\n        aln = WORKDIR + \"/{dataset}/subsets/{subset}/grasp_results/GRASP_ancestors.fa\",\n    output:\n        csv = WORKDIR + \"/{dataset}/subsets/{subset}/csv/{dataset}_{subset}_ancestors.csv\"\n    script:\n        CUSTOM_ANCESTOR_DIR + \"/add_annotations_from_ancestors_for_use_in_example_workflow.py\"\n</code></pre>"},{"location":"basic_snakemake/#snakemake-pipelines-only-rerun-what-is-needed","title":"Snakemake pipelines only rerun what is needed","text":"<p>Snakemake backtracks in this way, checking to see which rules need to be run in order to generate the required input for the <code>all</code> rule.</p> <p>If no data at all has been generated it will start with the very first step in the workflow, but if some steps have been completed then it does not need to regenerate them. This is an important concept in workflow management and saves us a lot of time.</p> <p>For example, in the <code>asr_curation</code> pipeline, all of the subset files use the <code>annotation_files</code> generated from going to the UniProt and BRENDA databases. So only one call to the databases needs to be made, and then all downstream analysis simply uses the generated output from the annotation steps.</p>"},{"location":"basic_snakemake/#a-closer-look-at-snakemake-rules","title":"A closer look at snakemake rules","text":"<p>Here are two <code>snakemake</code> rules. We can compare the differences in order to get a better sense of how to read <code>snakemake</code> rules in the <code>snakefile</code></p> <pre><code>rule validate_ids:\n   input:\n       WORKDIR + \"/{dataset}/csv/original/{dataset}_original.csv\"\n   output:\n       WORKDIR + \"/{dataset}/csv/validated/{dataset}_validated.csv\"\n   script:\n       \"scripts2/validate_ids.py\"\n</code></pre> <pre><code>rule infer_tree:\n    input:\n        WORKDIR + \"/{dataset}/subsets/{subset}/{dataset}_{subset}.aln\"\n\n    output:\n        WORKDIR + \"/{dataset}/subsets/{subset}/{dataset}_{subset}.nwk\"\n\n    shell:\n        \"FastTree {input} &gt; {output}\"\n</code></pre> <ul> <li><code>validate_ids</code> only uses the <code>dataset</code> wildcard as it is only run once per <code>dataset</code></li> <li><code>infer_tree</code> uses both the <code>dataset</code> and the <code>subset</code> wildcard as it is run once per <code>subset</code></li> <li>Both rules use the wildcards to name the output files - for example, in the example workflow, the phylogenetic tree generated for the dataset <code>kari_example_ec_1_1_1_86</code> and subset <code>eukaryotic_kari</code> is written to <code>/{dataset}/subsets/{subset}/{dataset}_{subset}.nwk</code> or <code>/kari_example_ec_1_1_1_86/subsets/eukaryotic_kari/kari_example_ec_1_1_1_86_eukaryotic_kari.nwk</code>. This means that files can easily be identified and will not overwrite each other. </li> <li><code>validate_ids</code> defines a <code>script</code> command - which tells <code>snakemake</code> to run the Python script <code>scripts/validate_ids.py</code>. You can look in the scripts folder to see exactly what is being run for each step.</li> <li><code>infer_tree</code> defines a <code>shell</code> commmand - which tells <code>snakemake</code> to run the defined command on the command line.</li> </ul>"},{"location":"customising_workflow/","title":"Customising workflows","text":""},{"location":"customising_workflow/#customising-the-annotations-scripts","title":"Customising the annotations scripts","text":"<p>The following rules / Python files stored in scripts are run at three different steps during the <code>asr_curation</code> pipeline </p> <ul> <li> <p>add_custom_annotations / scripts/add_custom_annotations.py' - run after all of the database annotations are retrived and before subsets are made</p> </li> <li> <p>add_annotations_from_alignment / scripts/add_annotations_from_alignment.py' - run after the sequences for a given subset have been aligned</p> </li> <li> <p>add_annotations_from_ancestors / scripts/add_annotations_from_ancestors.py' - run after the ancestors for a given subset have been predicted</p> </li> </ul> <p>Which script to use for which custom annotation is a decision as to what information you require to build the rules. For example, if your custom rules make use of aligned positions, this will obviously need to come after alignments have been generated.</p> <p>Currently only <code>add_custom_annotations</code> comes before the creation of subsets, so only annotations added from this custom script (or annotations from the database retrievals) can be used to make subsets. </p> <p>The files stored in <code>scripts</code> are template files - they will read in the data and write it out correctly, but not add any annotations.</p> <p>You can override these files by copying them to a new location and writing code to be annotated into your data. </p> <p>Within your config file you then specify the location of any custom script. </p> <p>See Defining the config files for more information.</p>"},{"location":"customising_workflow/#writing-code-for-new-annotation-scripts","title":"Writing code for new annotation scripts","text":"<p>You must keep the structure for the input and output the same but to help with this all three files define the following line -</p> <pre><code># IF CUSTOMISING THIS FILE PLACE CUSTOM CODE HERE\n</code></pre> <p>An example of how to write custom code is given in the <code>scripts/configs/example_custom_annotations</code> folder, and the example_workflow uses these files to add custom annotations to the datasets in the example_workflow</p> <p>There is a lot of code in <code>scripts/annot_functions</code> that is designed to do common annotation tasks.</p>"},{"location":"customising_workflow/#customising-annotations","title":"Customising annotations","text":""},{"location":"customising_workflow/#provided-annotation-functions","title":"Provided annotation functions","text":"<p>The following is a description of some of the provided annotation functions and how to incorporate them.</p> <p>All changes should be made in your <code>add_custom_annotations.py</code> file and you should add the custom location of this file to you <code>config</code> file.</p>"},{"location":"customising_workflow/#regnerating-annotation-files-without-making-new-alignments-or-trees","title":"Regnerating annotation files without making new alignments or trees","text":""},{"location":"customising_workflow/#create-a-top-column","title":"Create a top column","text":"<p>Creates a new column where if a value within a column is more than a given percentage, it will create a new column, labelled TOP__ with <code>True</code> or <code>False</code> values depending on whether a sequence has that annotation. <p>This can be useful for grouping together multiple annotations that differ from the dominant value in a column.</p> <p><code>annot_df = an.create_top_column(annot_df, 80)</code></p>"},{"location":"customising_workflow/#separate-out-the-note-values-from-the-feature-columns","title":"Separate out the note values from the feature columns","text":"<p>annot_df = an.separate_notes(annot_df)</p>"},{"location":"customising_workflow/#generating-embeddings-and-running-dbscan","title":"Generating embeddings and running DBSCAN","text":""},{"location":"customising_workflow/#add-embeddings","title":"Add embeddings","text":"<p>Generate embeddings using TM Vec. You need to download the checkpoint and config.json and point to their path in your <code>add_custom_annotations.py</code> code</p> <p>To download the files - <code>!wget https://users.flatironinstitute.org/thamamsy/public_www/tm_vec_cath_model.ckpt -q gwpy\\n\"</code> <code>!wget https://users.flatironinstitute.org/thamamsy/public_www/tm_vec_cath_model_params.json -q gwpy\"</code></p> <p>Then set their path in your file - </p> <pre><code>model_checkpoint_path = &lt;path_to_ckpt&gt;\nmodel_config_path = &lt;path_to_json&gt;\n</code></pre> <p>And then create the embeddings. This will store the generated embeddings in a local file, so that embeddings can be reused over subsequent runs of asr_curation.</p> <p>The default path is in the custom_annotations folder, in a pickle file - \"embeddings.pkl\", but this can be changed with the parameter <code>embedding_df_path</code> when using the following <code>process_and_store_embeddings</code> function</p> <p>annot_df = tm_vec_embed.process_and_store_embeddings(annot_df, 'Prot_T5', model_checkpoint_path, model_config_path)</p>"},{"location":"customising_workflow/#generate-dbscan-images","title":"Generate DBSCAN images","text":"<p>Generate a DBSCAN coverage image</p> <pre><code>db.generate_dbscan_coverage(annot_df, 'Prot_T5 Embed Encoded', \n    f\"{snakemake.input.custom_dir}/{snakemake.wildcards.dataset}_dbscan_coverage\" )\n</code></pre> <p>Here we provide the <code>dataframe</code> name, the name of the column to find the embeddings in (by default 'Prot_T5 Embed Encoded' if you are using Prot_T5 in the previous step) and a prefix for the output files</p> <p>You can also pass columns that you do not wish to include in the DBSCAN analysis using the parameter <code>skip_cols</code> <pre><code>db.generate_dbscan_coverage(annot_df, 'Prot_T5 Embed Encoded', \n    f\"{snakemake.input.custom_dir}/{snakemake.wildcards.dataset}_dbscan_coverage_no_ft\",\n    skip_cols = ['ft_var_seq||', 'ft_variant||', 'ft_conflict||', 'ft_chain||', 'ft_crosslnk||', 'ft_carbohyd||', 'ft_init_met||' , 'ft_mod_res||', 'ft_lipid||', 'ft_transit||', 'ft_compbias||', 'ft_domain||', 'ft_motif||',  'ft_region||', 'ft_repeat||', 'ft_zn_fing||', 'ft_binding||', 'ft_topo_dom||', 'ft_act_site||'])\n</code></pre></p>"},{"location":"customising_workflow/#full-minimal-example-of-generating-embeddings-and-running-dbscan-outlier-detection","title":"Full minimal example of generating embeddings and running DBSCAN outlier detection","text":"<p>The following can be created as <code>add_custom_annotations.py</code> to generate embeddings. Make sure to specify that you are using a custom annotations file in your <code>config</code> file  and to update the paths to the TM-Vec checkpoint / config.</p> <pre><code>import os\nimport annot_functions as an\nimport get_funfams as ff\nimport map_to_cdd as m2c\nimport seqcurate as sc\nimport pandas as pd\nimport numpy as np\nimport add_embeddings as embed\nimport add_tm_vec_embeddings as tm_vec_embed\nimport create_itol_files as itol\nimport create_dbscan_coverage as db\n\n\nannot_df = pd.read_csv(snakemake.input.csv)\n\n# Create TOP column for high scoring values\n\nannot_df = an.create_top_column(annot_df, 80)\n\n# Separate out the note values from the feature columns\n\nannot_df = an.separate_notes(annot_df)\n\n# Add embeddings\n\nmodel_checkpoint_path = &lt;path_to_checkpoint&gt;\nmodel_config_path = &lt;path_to_config&gt;\n\nannot_df = tm_vec_embed.process_and_store_embeddings(annot_df, 'Prot_T5', model_checkpoint_path, model_config_path)\n\n# Generate DBSCAN images\ndb.generate_dbscan_coverage(annot_df, 'Prot_T5 Embed Encoded', \n    f\"{snakemake.input.custom_dir}/{snakemake.wildcards.dataset}_dbscan_coverage\" )\n\n\ndb.generate_dbscan_coverage(annot_df, 'Prot_T5 Embed Encoded', \n    f\"{snakemake.input.custom_dir}/{snakemake.wildcards.dataset}_dbscan_coverage_no_ft\",\n    skip_cols = ['ft_var_seq||', 'ft_variant||', 'ft_conflict||', 'ft_chain||', 'ft_crosslnk||', 'ft_carbohyd||', 'ft_init_met||' , 'ft_mod_res||', 'ft_lipid||', 'ft_transit||', 'ft_compbias||', 'ft_domain||', 'ft_motif||',  'ft_region||', 'ft_repeat||', 'ft_zn_fing||', 'ft_binding||', 'ft_topo_dom||', 'ft_act_site||'])\n\nannot_df.to_csv(snakemake.output[0], index=False)\n</code></pre>"},{"location":"defining_files/","title":"Defining input files","text":""},{"location":"defining_files/#defining-the-config-files","title":"Defining the config files","text":"<p>Every snakemake pipeline needs to be run with a specific config file - a YAML file with a .yaml extension.</p> <p>For example this call uses the <code>--configfile</code> flag to set the location to the <code>example_config.yaml</code> file stored in the asr_curation repository</p> <pre><code>snakemake --cores 1 --configfile ./asr_curation/config/example_config.yaml\n</code></pre> <p>A config file must define the following parameters</p> <ul> <li> <p>workdir - where the <code>datasets</code> folder and all of the created data will be stored</p> </li> <li> <p>fastadir - where the fasta files are stored</p> </li> <li> <p>subdir - where the <code>subset</code> files are stored - they must match the name of the .fasta file they refer to but contain a .subset extension</p> </li> <li> <p>annotation_cols - this specifies which columns in the annotation file should be written to the FigTree annotation file</p> </li> <li> <p>blocked_datasets - a list of datasets that will not be run - meaning you can keep the FASTA and .subset file in the fastadir and subdir and just skip over the generation of these files. An empty list <code>[]</code> will run every fasta and matching .subset file</p> </li> </ul> <p>A config file can contain the following optional parameters</p> <p>These define the custom annotations that can be added. </p> <p>See Customising the annotations scripts for more details</p> <ul> <li>custom_dir - if you want to use a custom <code>add_custom_annotations.py</code> file then set the directory location here and place a custom file name <code>add_custom_annotations.py</code> into this folder</li> <li>custom_align_dir - if you want to use a custom <code>add_annotations_from_alignment.py</code> file then set the directory location here and place a custom file name <code>add_annotations_from_alignment.py</code> into this folder</li> <li>custom_align_dir - if you want to use a custom <code>add_annotations_from_ancestors.py</code> file then set the directory location here and place a custom file name <code>add_annotations_from_ancestors.py</code> into this folder</li> </ul>"},{"location":"defining_files/#example-config-file","title":"Example config file","text":"<p>Here is an example config file from the example_workflow in the <code>asr_curation</code> repository</p> <pre><code># Main working directory\nworkdir: \"workflows/example_workflow\"\n\n# Store all FASTA files here\nfastadir: \"workflows/example_workflow/fasta\"\n\n# Each FASTA file needs an according .subset file in this folder (test1.fasta -&gt; test1.subset)\nsubdir: \"workflows/example_workflow/subset_rules\"\n\n# Which columns should we plot in the summary documents?\nannotation_cols: ['lineage_superkingdom', 'xref_supfam', 'xref_panther', 'KARI_Class', 'ec']\n\n# Directories for custom annotations\ncustom_dir : \"scripts/example_custom_annotations\"\ncustom_align_dir: \"scripts/example_custom_annotations\"\n\n\n# Block a data set from being run (you can still keep the FASTA and .subset file in the fastadir and subdir)\nblocked_datasets : []\n</code></pre>"},{"location":"defining_files/#defining-the-subset-files","title":"Defining the subset files","text":"<p>Subset files need to be in the format </p> <p><code>&lt;subset_name&gt; = &lt;column_name&gt; : &lt;string&gt; $ &lt;other_column_name&gt; : NOT &lt;string&gt;</code></p> <p>Where <code>subset_name</code> can be any name you wish to call the subset, <code>column_name</code> and <code>other_column_name</code> are columns that appear in the final annotation file (see /csv/custom/*.csv) and <code>string</code> is a term that you wish to include or exclude.</p> <p>For each row / sequence in your annotation file, if the given column contains the string (or substring) then it will be included.</p> <p>If you use the <code>NOT</code> modifier, for each row / sequence in your annotation file, if the given column contains the string (or substring) then it will not be included.</p> <p>You can define multiple subsets within the one file.</p> <pre><code># THIS IS A COMMENT LINE.\n\neukaryotic_kari = lineage_superkingdom : Eukaryota $ protein_name : NOT Deleted, Merged $ Non_AA_Character : FALSE\n\nclassI_kari = KARI_Class : Class_1 $ protein_name : NOT Deleted, Merged $ Non_AA_Character : FALSE\n\nall_kari = *\n</code></pre>"},{"location":"defining_files/#including-all-sequences-in-a-subset","title":"Including all sequences in a subset","text":"<p>The default way to include every row / sequence is </p> <p><code>&lt;subset_name&gt; = *</code></p> <p>For example,</p> <p><code>all = *</code> </p>"},{"location":"detailed_explanation/","title":"Snakemake diagram","text":""},{"location":"detailed_explanation/#snakemake-diagram","title":"Snakemake diagram","text":"<p>The following diagram is the overall steps that have to be run by <code>snakemake</code> to generate the example data.</p> <p>Diagram</p> <p>Note that in this case, the KARI data had three subsets defined, so <code>create_subsets</code> and all of the other steps downstream from this step are run three times, while the ALS data only has one subset defined and so <code>create_subsets</code> is only run once.</p> <p></p>"},{"location":"example/","title":"Running example data","text":""},{"location":"example/#running-example-data","title":"Running example data","text":"<p>The repository contains example data that can be inspected and rerun.</p> <p>You can see the config file in <code>config/example_config.yaml</code></p> <p>The first few lines of this config file define where data will be stored and where the required input files are</p> <p><code>config/example_config.yaml</code> <pre><code># Main working directory\nworkdir: \"workflows/example_workflow\"\n\n# Store all FASTA files here\nfastadir: \"workflows/example_workflow/fasta\"\n\n# Each FASTA file needs an according .subset file in this folder (test1.fasta -&gt; test1.subset)\nsubdir: \"workflows/example_workflow/subset_rules\"\n</code></pre></p> <p>All of these are directories within the main <code>asr_curation</code> repository.</p>"},{"location":"example/#fasta-directory","title":"FASTA directory","text":"<p>You can see that the FASTA directory <code>fastadir</code> at <code>workflows/example_workflow/fasta</code> contains two FASTA files </p> <ul> <li>als_example_ec_2_2_1_6.fasta</li> <li>kari_example_ec_1_1_1_86.fasta</li> </ul>"},{"location":"example/#subset-directory","title":"Subset directory","text":"<p>And the subset directory <code>subdir</code> at <code>workflows/example_workflow/subset_rules</code> contains a matching set of subset_rules for each dataset</p> <ul> <li>als_example_ec_2_2_1_6.subset</li> <li>kari_example_ec_1_1_1_86.subset</li> </ul> <p>The ALS subset file contains a single subset -</p> <pre><code>als_interpro_IPR012782 = Non_AA_Character : False $ protein_name : NOT Deleted, Merged $ xref_interpro : IPR012782\n</code></pre> <p>And the KARI subset file contains three subsets - </p> <p>One of these will only contain Eukaryotic KARI sequences, one will only contains Class I KARI sequences, and one will  contain all of the KARI sequences</p> <pre><code>eukaryotic_kari = lineage_superkingdom : Eukaryota $ protein_name : NOT Deleted, Merged $ Non_AA_Character : FALSE\n\nclassI_kari = KARI_Class : Class_1 $ protein_name : NOT Deleted, Merged $ Non_AA_Character : FALSE\n\nall_kari = *\n</code></pre> <p>Therefore if we look at the generated data, we will see -</p>"},{"location":"example/#running-it-via-the-tests","title":"Running it via the tests","text":"<p>You can validate that everything is working correctly by running a small example workflow locally. </p> <p>There is a test within <code>test/test_snakemake.py</code> called <code>test_snakemake_pipeline</code></p> <p>This file only contains a small number of sequences so that it can be run quickly.</p> <p>You can run this test by - </p> <p><code>conda activate asr_curation</code></p> <p><code>pytest test/test_snakemake.py</code></p> <p>This will run the entire <code>snakemake</code> pipeline from end to end, including any database retrieval.</p> <p>The output folders stay after the test is run so that you can inspect them, but are deleted every time the test is rerun.</p> <p>See <code>test_snakemake_pipeline</code>  in <code>test/test_snakemake.py</code> for more details about this test and <code>test/files/config/test_config.yaml</code> for the full details of where output folders, fasta directories, and subset directories are set.</p> <p>There is also another test within <code>test/test_snakemake.py</code> called <code>test_snakemake_pipeline_with_existing_annotation_file</code>  that does the same test without deleting the original annotations file - meaning that no calls to external databases need to be made and only the subset generation, alignment, tree inference, and ancestor prediction is rerun - making this even quicker to test the pipeline.</p>"},{"location":"example/#rerunning-the-entire-example_workflow","title":"Rerunning the entire example_workflow","text":"<p>You can also delete the entire output of the example_workflow locally and then run it again to check that you can regenerate the data.</p> <p>Note that these files contain substantially more data than the tests, so will take longer to run.</p> <p>Remove the <code>datasets</code> directory stored in <code>workflows/example_workflow</code>. Make sure to keep the <code>fastadir</code> and <code>subdir</code> directories <pre><code>rm -rf workflows/example_workflow/datasets/\n</code></pre></p> <p>Rerun the snakemake pipeline to re-generate the data</p> <p>Warning</p> <p>This will retrigger calls to the UniProt and BRENDA databases, so the annotations may not be identical to those stored in this repository as sequences get updated in these databases. Any changes you make to this files are excluded from the Git repository - so a fresh git pull will overwrite any changes you make to the example data.</p>"},{"location":"install/","title":"Installation and quick start","text":""},{"location":"install/#installation-guide","title":"Installation guide","text":"<ol> <li> <p>Clone this repository to your desktop</p> <pre><code>git clone https://github.com/gabefoley/asr_curation.git\n</code></pre> </li> <li> <p>Create a conda environment</p> <pre><code>conda create -n asr_curation python=3.9\n</code></pre> </li> <li> <p>Activate the conda environment</p> <pre><code>conda activate asr_curation\n</code></pre> </li> <li> <p>Install the required Python packages</p> <pre><code>pip install -r requirements.txt\n</code></pre> </li> <li> <p>Install the following so that they are callable from the command line</p> <ol> <li> <p>mafft - callable as <code>mafft</code></p> </li> <li> <p>FastTree - callable as <code>FastTree</code></p> </li> <li> <p>GRASP - callable as <code>grasp</code></p> </li> <li> <p>FigTree (Optional - for viewing trees with generated annotation files)</p> </li> </ol> </li> </ol>"},{"location":"install/#quick-start","title":"Quick start","text":"<p>To run an <code>asr_curation</code> pipeline you need to define -</p> <ul> <li>a <code>config.yaml</code> file that defines where to look for the <code>fasta</code> and <code>subset</code> files. See Defining the config files for further details. )</li> <li>a <code>fasta</code> file containing all the sequences you wish to include in the pipeline</li> <li>a <code>subset</code> file containing rules for subsetting the data. See Defining the subset files for further details. </li> </ul> <p>You can then run snakemake directly by calling it from within the <code>asr_curation</code> environment we created and activated earlier.</p> <p>You need to specify the number of cores to run <code>snakemake</code> with and point to the <code>config.yaml</code> file. <pre><code>snakemake --cores 1 --configfile ./asr_curation/config/example_config.yaml\n</code></pre></p>"},{"location":"output/","title":"Explanation of output","text":""},{"location":"output/#explanation-of-output","title":"Explanation of output","text":"<p>The following image shows the directory structure of the output directories after running the example data.</p> <p>Remember that the example workflow defined two datasets - one with three subsets, and one with a single subset.</p> <p>Hence the output directories contain </p> <ul> <li>the original <code>fasta</code> and <code>subset</code> directories (red)</li> <li>annotation files that are built for each dataset (green)</li> <li>subset specific alignment and tree files (orange)</li> <li>subset specific annotation files (yellow)</li> <li>subset specific ancestor files (blue)</li> </ul> <p> </p> Output directory structure of example data"},{"location":"tips_and_tricks/","title":"Tips and tricks","text":"<p>:)</p>"},{"location":"visualisation/","title":"Visualisation","text":""}]}