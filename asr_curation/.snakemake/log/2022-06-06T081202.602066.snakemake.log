Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job stats:
job                               count    min threads    max threads
------------------------------  -------  -------------  -------------
add_annotations_from_alignment        3              1              1
add_custom_annotations                2              1              1
align_seqs                            3              1              1
all                                   1              1              1
clean_subset_summary                  3              1              1
clean_summary_document                2              1              1
compile_summary_document              3              1              1
create_annotation_file                3              1              1
create_annotations                    2              1              1
create_dataset_summary                2              1              1
create_subset_document                3              1              1
create_subset_summary                 3              1              1
create_subsets                        3              1              1
get_brenda_annotations                2              1              1
get_uniprot_annotations               2              1              1
total                                37              1              1

Select jobs to execute...

[Mon Jun  6 08:12:02 2022]
rule create_annotations:
    input: workflows/example_workflow/fasta/uniprot_ec_1_1_1_86_filtered_reviewed_yes.fasta
    output: workflows/example_workflow/datasets/uniprot_ec_1_1_1_86_filtered_reviewed_yes/csv/original/uniprot_ec_1_1_1_86_filtered_reviewed_yes_original.csv
    jobid: 8
    reason: Missing output files: workflows/example_workflow/datasets/uniprot_ec_1_1_1_86_filtered_reviewed_yes/csv/original/uniprot_ec_1_1_1_86_filtered_reviewed_yes_original.csv
    wildcards: dataset=uniprot_ec_1_1_1_86_filtered_reviewed_yes
    resources: tmpdir=/var/folders/xs/24s9hwqd191f2x7rhdy6_ryc0000gr/T

[Mon Jun  6 08:12:05 2022]
Finished job 8.
1 of 37 steps (3%) done
Select jobs to execute...

[Mon Jun  6 08:12:05 2022]
rule get_uniprot_annotations:
    input: workflows/example_workflow/datasets/uniprot_ec_1_1_1_86_filtered_reviewed_yes/csv/original/uniprot_ec_1_1_1_86_filtered_reviewed_yes_original.csv
    output: workflows/example_workflow/datasets/uniprot_ec_1_1_1_86_filtered_reviewed_yes/csv/uniprot/uniprot_ec_1_1_1_86_filtered_reviewed_yes_uniprot.csv
    jobid: 7
    reason: Missing output files: workflows/example_workflow/datasets/uniprot_ec_1_1_1_86_filtered_reviewed_yes/csv/uniprot/uniprot_ec_1_1_1_86_filtered_reviewed_yes_uniprot.csv; Input files updated by another job: workflows/example_workflow/datasets/uniprot_ec_1_1_1_86_filtered_reviewed_yes/csv/original/uniprot_ec_1_1_1_86_filtered_reviewed_yes_original.csv
    wildcards: dataset=uniprot_ec_1_1_1_86_filtered_reviewed_yes
    resources: tmpdir=/var/folders/xs/24s9hwqd191f2x7rhdy6_ryc0000gr/T

[Mon Jun  6 08:12:36 2022]
Error in rule get_uniprot_annotations:
    jobid: 7
    output: workflows/example_workflow/datasets/uniprot_ec_1_1_1_86_filtered_reviewed_yes/csv/uniprot/uniprot_ec_1_1_1_86_filtered_reviewed_yes_uniprot.csv

RuleException:
CalledProcessError in line 136 of /Users/uqgfoley/Documents/asr_curation_moved/asr_curation/Snakefile:
Command 'set -euo pipefail;  /Users/uqgfoley/opt/miniconda3/envs/freshandclean/bin/python /Users/uqgfoley/Documents/asr_curation_moved/asr_curation/.snakemake/scripts/tmp2ze38f4e.get_uniprot_annotations_async.py' returned non-zero exit status 1.
  File "/Users/uqgfoley/Documents/asr_curation_moved/asr_curation/Snakefile", line 136, in __rule_get_uniprot_annotations
  File "/Users/uqgfoley/opt/miniconda3/envs/freshandclean/lib/python3.9/concurrent/futures/thread.py", line 52, in run
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: .snakemake/log/2022-06-06T081202.602066.snakemake.log
